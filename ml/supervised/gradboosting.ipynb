{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind gradient boosting is that we start off with any random model, usually being an unoptimized tree, and then, based off its predictions, optimize the bias. Since most models rely on $\\hat{y}+bias$ to get the real prediction, we're pretty much just approximating our function, but through the bias term rather than coefficients.\n",
    "\n",
    "#### Intuitive description of the algorithm\n",
    "- The starting model, based on which we are going to optimize the bias, could be anything, even the mean value across all predictions. By plugging $X$ we get initial predictions $\\hat{y}$;\n",
    "- We calculate residuals $e=y-\\hat{y}$. Now we build a \"weak learner\" for the bias term. For example, it could be a decision tree that predicts the bias. Not that in the regression problem if there are more than one value left in the leaf node, we take the average to yeild the prediction. For more generalization we plug $\\eta$ as a learning rate parameter to the predicted residuals;\n",
    "- We calculate new predictions based on $\\hat{y}+\\eta\\times e_1$ and then repeat the process until we reach $M$ iterations or a desired quality\n",
    "\n",
    "#### Mathematically strictier description\n",
    "- Initially we want to find such an arbitrary function $\\hat{f}(x)$ that minimizes $y\\approx\\hat{f}(x)$. That means that $\\hat{f}(x)=argmin_{f(x)}L(y,f(x))$, where $L(y,f)$ is some **differentiable** loss function\n",
    "- Since the range of possible functions approximating $f(x)$ is infinite, we limit the problem to a family of functions $f(x,\\theta), \\theta\\in R^d$ (for example a decision tree) and transform the problem to finding best $\\theta$ instead of some \"function\": $\\hat{\\theta}=argmin_{\\theta}E_{x,y}[L(y,f(x,\\theta))]$, where $E$ - expected value or mean. \n",
    "    > **Improtant reminder** is that just like in every other NN we get best parameters by applying gradient descend with some multiplication by $\\eta\\in(0,1]$, i.e. each parameter actually consists of a negative sum of losses with preadded initial value of the parameter. Therefore, for better intuition we can rewrite the optimization problem as $\\hat{\\theta}=\\sum^N_{i=1}L(y_i,f(x_i,\\hat{\\theta}))$\n",
    "- Right now it seems that GBMs are nothing more but a fancier way to describe regular linear models. To elevate confusion it is important to say that GBMs approximate a function as a sum of incremental improvements, **each being a function**, i.e. $\\hat{f}(x)=\\sum_{i=0}^M\\hat{f_i}(x)$, where $\\hat{f}(x)$ is limited by some function family $\\hat{f}(x)=h(x,\\theta)$. Moreover on every step of finding another function (from now on we will also refer to them as \"models\") we also need to select an optimal $\\rho\\in R$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "\n",
    "class SquareLoss(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return 0.5 * np.power((y - y_pred), 2)\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        return -(y - y_pred)\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def acc(self, y, p):\n",
    "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - (y / p) + (1 - y) / (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "class gbm():\n",
    "    \n",
    "    def  __init__(self, eta, m, min_samples, min_impurity, \n",
    "                  max_depth, is_reg) -> None:\n",
    "        \"\"\" \n",
    "        eta: learning rate\n",
    "        m: number of weak learners\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.m = m\n",
    "        self.min_samples = min_samples\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        self.is_reg = is_reg\n",
    "\n",
    "        self.loss = SquareLoss()\n",
    "        if not self.is_reg:\n",
    "            self.loss = CrossEntropy()\n",
    "\n",
    "        # initialize estimators, e.g. trees\n",
    "        self.tree_estimators = []\n",
    "        for _ in range(self.m):\n",
    "            self.tree_estimators.append(\n",
    "                DecisionTreeRegressor(\n",
    "                    min_samples_split=self.min_samples,\n",
    "                    min_impurity_decrease=self.min_impurity,\n",
    "                    max_depth=self.max_depth)\n",
    "            )\n",
    "        \n",
    "    def _softmax(self, values):\n",
    "        return np.exp(values) /\\\n",
    "            np.expand_dims(np.sum(np.exp(values, axis=1)),axis=1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(y.shape,y.mean())\n",
    "        for i in tqdm(range(self.m)):\n",
    "            grad = self.loss.gradient(y, y_pred)\n",
    "            self.tree_estimators[i].fit(X, grad)\n",
    "            weak_learner = self.tree_estimators[i].predict(X)\n",
    "            y_pred -= self.eta * weak_learner\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.array([])\n",
    "\n",
    "        # run the sample through ensemble of weak learners\n",
    "        # recursively from bottom to the initial prediction\n",
    "        for tree in self.tree_estimators: \n",
    "            weak_learner = self.eta * tree.predict(X)\n",
    "            y_pred = -weak_learner if \\\n",
    "                not y_pred.any() else y_pred - weak_learner\n",
    "        \n",
    "        if not self.is_reg:\n",
    "            # apply softmax to predicted estimated values\n",
    "            # to transform values into [0,1] \"probability\" dist\n",
    "            y_pred = np.argmax(self._softmax(y_pred),axis=1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBM is a universal approach to both regression and classifcation problems, therefore we need to build additional child classes for each type of the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gbmRegressor(gbm):\n",
    "    def __init__(self, n_estimators=200, eta=0.5, min_samples_split=2,\n",
    "                 min_var_red=1e-7, max_depth=4):\n",
    "        super().__init__(m=n_estimators, \n",
    "            eta=eta, \n",
    "            min_samples=min_samples_split, \n",
    "            min_impurity=min_var_red,\n",
    "            max_depth=max_depth,\n",
    "            is_reg=True)\n",
    "\n",
    "class gbmClassifier(gbm):\n",
    "    def __init__(self, n_estimators=200, eta=.5, min_samples_split=2,\n",
    "                 min_info_gain=1e-7, max_depth=2):\n",
    "        super().__init__(m=n_estimators, \n",
    "            eta=eta, \n",
    "            min_samples=min_samples_split, \n",
    "            min_impurity=min_info_gain,\n",
    "            max_depth=max_depth,\n",
    "            is_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare, for instance, our GBM for regression with Scikit-learn implementation on some arbitrary regression dataset. We can see that $R^2=1-\\frac{\\text{Sum of squares of residuals from regression}}{\\text{Sum of squares of residuals against the average}}$, which explaines how much space variance in regression takes up in the explained variance in the dataset is slightly lower than in Scikit-learn, which means that a $MSE_{friedman}$ used as a criterion for the split quality in Scikit-learn's GBM implementation is better than the regular squared error used in the Tree regressor by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 184.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.3920367384803918 :: MSE: 11929.734155842236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# our own implementation\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "customReg = gbmRegressor(eta=0.1, n_estimators=100,\n",
    "                         min_samples_split=2)\n",
    "customReg.fit(X_train, y_train)\n",
    "predictions = customReg.predict(X_test)\n",
    "print(f'R^2: {r2_score(y_test, predictions)} :: ' \\\n",
    "      f'MSE: {mean_squared_error(y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.43542564320925925 :: MSE: 11078.337152946428\n"
     ]
    }
   ],
   "source": [
    "# sklearn implementation\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "predictions_sklearn = reg.predict(X_test)\n",
    "print(f'R^2: {r2_score(y_test, predictions_sklearn)} :: ' \\\n",
    "      f'MSE: {mean_squared_error(y_test, predictions_sklearn)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
