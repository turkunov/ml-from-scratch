{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTR (Learning To Rank) is a subset of supervised ML models that are meant to solve the ranking problem. Unlike classification and regression problems these models entail the usage of different niche metrics that are going to be explained below.\n",
    "\n",
    "#### Ranking metrics\n",
    "* `Acccuracy@k`, `Recall@k` - metrics that are used to compare binary predictions of $0$ (not relevant) and $1$ (relevant) agaist some ground truth based on $k$ first entries\n",
    "* `AP@k` - average precision. Suppose we have some distribution of `Accuracy@k` and `Recall@k`. We plot these values against each other on the XY plane. Then we calculate an area below it, to get a total probability of getting those accuracies and recall scores. Therfore AP can be also referred as AUC of the precision-recall curve. Thus, $AP@k=\\frac{1}{\\#\\text{relevant docs}}\\sum_{i=1}^K \\text{accuracy}@i\\times[relevancy_i=1]$\n",
    "* `MAP@k` - mean AP. We just calculate the mean over all AP's calculated across samples for ranking\n",
    "* `CMC@k` - probability of having a relevant document in first $k$ entries\n",
    "* `NDCG@k` = $\\frac{DCG@k}{IDCG@k}$ (Normalized Discounted Cumulative Gain). The $DCG@k=\\sum_{i=1}^K\\frac{\\text{relevancy}_i=1}{log(1+i)}$, whereas $IDCG@k$ is the same formula, but calculated over the ideal relevancy distribution in $k$ entries (e.g. all first k elements must be relevant). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
